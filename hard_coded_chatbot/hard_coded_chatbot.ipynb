{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refereces:\n",
    "- https://www.google.com/search?q=how+to+use+tkinter+to+create+a+chatbot+in+python&oq=how+to+use+tkinter+to+create+a+chatbot&aqs=chrome.1.69i57j33.44216j0j4&sourceid=chrome&ie=UTF-8#kpvalbx=_VJZOX5znAquIytMPkfS2iAE37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "root=Tk()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "df=pd.read_csv(\"chatbot_data_Q&A - basic_python_questions.csv\").dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "answer      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()# check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data?</td>\n",
       "      <td>Based on the definition from google : facts an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is data science?</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is big data?</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>definition of big data</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer\n",
       "0                    data?  Based on the definition from google : facts an...\n",
       "1    what is data science?  Data science is an inter-disciplinary field th...\n",
       "2             data science  Data science is an inter-disciplinary field th...\n",
       "3        what is big data?  In the data science domain, big data usually r...\n",
       "4  definition of big data   In the data science domain, big data usually r..."
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()# take a look at the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 2)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some uncompleted questions\n",
    "# if a question or answers end with \":\" \n",
    "questions=[]\n",
    "for i in df.question:\n",
    "    if i[-1]==\":\":\n",
    "        questions.append(None)\n",
    "    else:\n",
    "        questions.append(i)\n",
    "        \n",
    "        \n",
    "# remove some uncompleted answers\n",
    "answers=[]\n",
    "for i in df.answer:\n",
    "    if i[-1]==\":\":\n",
    "        answers.append(None)\n",
    "    else:\n",
    "        answers.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange a little bit\n",
    "df['answer']=answers\n",
    "df['question']=questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data?</td>\n",
       "      <td>Based on the definition from google : facts an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is data science?</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is big data?</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>definition of big data</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer\n",
       "0                    data?  Based on the definition from google : facts an...\n",
       "1    what is data science?  Data science is an inter-disciplinary field th...\n",
       "2             data science  Data science is an inter-disciplinary field th...\n",
       "3        what is big data?  In the data science domain, big data usually r...\n",
       "4  definition of big data   In the data science domain, big data usually r..."
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(axis=0).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 2)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]\\|', \"\", text)\n",
    "    text=text.replace(\"[\\'\",\"\").replace(\"\\n\",\" \").replace(\"']\",\" \").replace('[\"',\"\").replace('\"]',\"\").replace(\"it\\'s\",\"it's \").replace(\"\\', \\'\",\"\")\n",
    "    text=text.replace(\"\\',\",\"\").replace(\"it\\'s\",\"it is \").replace(\"it\\\\\\'s\",\"it is\").replace(\" \\\\\",\" \")\n",
    "    text=text.replace('\",',\" \").replace(\"\\',\",\"\").replace( \":\\',\",\"\").replace(\"here\\'s\",\"\").replace(\":\",\"\").replace(',\"',\"\")\n",
    "    text=text.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'s\",\"'s\")\n",
    "    text=re.sub(r\"let's\", \"let us\", text)\n",
    "    text = text.replace(\"\\'s\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#declare answers and questions\n",
    "questions=df.question\n",
    "answers=df.answer\n",
    "# Cleaning the questions\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "# Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definition of data visualization, what is data visualization?',\n",
       " 'what tools do professional data scientists use? ',\n",
       " 'mian tools in data science',\n",
       " 'how to install git',\n",
       " 'what is conda and anaconda?']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_questions[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contains  punctuation\n",
    "#sw_list = stopwords.words('english')\n",
    "sw_list = list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'said',\"'s\", \"also\",'one',\"n't\",'com', '-', '–','--' ,\n",
    "'—', '_']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    return punctuation_removed\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "# import libraries\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    #tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    # stemming\n",
    "    stemming_string=list(map(stemming,processed_string))\n",
    "    # lemmatization\n",
    "    lemma_string=list(map(lemmatization,stemming_string))\n",
    "    \n",
    "    return lemma_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function above to process the data \n",
    "cleaned_questions=data_preprocessing(clean_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['data'], ['what', 'be', 'data', 'scienc'], ['data', 'scienc'], ['what', 'be', 'big', 'data'], ['definit', 'of', 'big', 'data'], ['big', 'data'], ['famou', 'data', 'scientist'], ['what', 'be', 'matplotlib'], ['what', 'be', 'seaborn'], ['what', 'be', 'panda']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_questions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function of NLP for single line\n",
    "def NLP(text):\n",
    "    cleaned_question=clean_text(text)\n",
    "    processed_question=process_data(cleaned_question)\n",
    "    stemming_question=stemming(processed_question)\n",
    "    lemma_question=lemmatization(stemming_question)\n",
    "    return lemma_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Greeting , goodbye and thank you you are welcome\n",
    "greeting=['hey', 'hi', 'hello', 'hey man', 'hi how are you', 'how are you','how is it going', \n",
    "          'nice to meet you', 'how are you doing', 'what is up', 'what is new', \n",
    "          'what is going on', 'how is everything', 'how are things', 'how is life', \n",
    "          'how is your day', 'how is your day going', 'good to see you', 'nice to see you']\n",
    "goodbye=[\"see you\",\"bye\",\"byebye\",\"goodbye\"]\n",
    "\n",
    "thankyou=[\"thanks\",\"thank you\", \"thank you very much\"]\n",
    "yourwelcome=[\"you are welcome ^.^\",\"my pleasure!\",\"I am happy to help you!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# from time import sleep\n",
    "# print(\"=============# Hi There my name is Chacha. Let's Talk #=============\")\n",
    "\n",
    "# while(True):\n",
    "    \n",
    "    \n",
    "#     question = input(\"You: \")\n",
    "    \n",
    "#     if question.strip().lower() in goodbye:\n",
    "#         print(\"Chacha: \"+ \"Bye\")\n",
    "#         break \n",
    "        \n",
    "#     if  question.strip().lower() in greeting:\n",
    "#         print(\"Chacha: \"+ random.choice(greeting).capitalize()+\"\\n\")\n",
    "#         continue\n",
    "        \n",
    "#     if question.strip().lower() in thankyou:\n",
    "#         print(\"Chacha: \"+ random.choice(yourwelcome).capitalize()+\"\\n\")\n",
    "#         continue\n",
    "#     # NLP\n",
    "#     pro_text= NLP(question)\n",
    "\n",
    "#     # to find which row has intersection with the words from question you asked\n",
    "#     # which means to find the who have common elements between your question and the data\n",
    "#     inter_list=[]\n",
    "#     for i in cleaned_questions:\n",
    "#         if (set(pro_text) & set(i)):\n",
    "#             inter_list.append((list(set(pro_text) & set(i)),cleaned_questions.index(i)))\n",
    "\n",
    "#     # remove stop words  \n",
    "#     new_inter_list=[]\n",
    "#     for i in range(len(inter_list)):\n",
    "#         for j in inter_list[i][0]:\n",
    "#             if j not in stopwords.words('english'):\n",
    "#                 new_inter_list.append(inter_list[i])\n",
    "\n",
    "#     # find the max length of common elements\n",
    "#     lengths=[len(new_inter_list[i][0]) for i in range(len(new_inter_list)) ]\n",
    "#     max_length=max(lengths)\n",
    "\n",
    "\n",
    "#     if len(lengths)>0:\n",
    "#         indexes=[]\n",
    "#         # find all the index whose correspondiong question data have the most common elements\n",
    "#         for i in range(len(new_inter_list)):\n",
    "#             if len(new_inter_list[i][0])==max_length:\n",
    "#                 indexes.append(new_inter_list[i][1])\n",
    "#     else:\n",
    "#         print(\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\")\n",
    "\n",
    "\n",
    "#     ratios=[]\n",
    "#     for i in list(set(indexes)):\n",
    "#         ratio=len(pro_text)/len(questions.iloc[i])\n",
    "#         ratios.append((ratio,i))\n",
    "   \n",
    "#     max_ratios=max([ratios[i][0] for i in range(len(ratios))])\n",
    "\n",
    "    \n",
    "#     final_indexes=[]\n",
    "#     for i in range(len(ratios)):\n",
    "#         if ratios[i][0]==max_ratios:\n",
    "#             final_indexes.append(ratios[i][1])\n",
    "\n",
    "\n",
    "#     if len(final_indexes)>0:\n",
    "#         # to randomly find an answer based on the index\n",
    "#         answer_index=random.choice(final_indexes)\n",
    "            \n",
    "#         print(\"\\n\"+\"Chacha: \"+ answers.iloc[answer_index]+\"\\n\")\n",
    "#     else:\n",
    "#         print(\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def func(event):\n",
    "     print(\"You hit return.\")\n",
    "def send(event=None):\n",
    "    send=\"You: \"+e.get()\n",
    "    txt.insert(END,\"\\n\"+send)\n",
    "    txt.see(END+\"\\n\")\n",
    "    \n",
    "    if e.get().strip().lower() in goodbye:\n",
    "        txt.insert(END,\"\\n\"+\"Chacha: \"+\"Byebye! see you next time! \"+\"\\n\")  \n",
    "        e.delete(0,END)\n",
    "        txt.see(END)\n",
    "\n",
    "    elif  e.get().strip().lower() in greeting:\n",
    "        txt.insert(END,\"\\n\"+\"Chacha: \"+ random.choice(greeting).capitalize()+\"\\n\")\n",
    "        e.delete(0,END)\n",
    "        txt.see(END)\n",
    "\n",
    "    elif e.get().strip().lower() in thankyou:\n",
    "        txt.insert(END,\"\\n\"+\"Chacha: \"+ random.choice(yourwelcome).capitalize()+\"\\n\")\n",
    "        e.delete(0,END)\n",
    "        txt.see(END)\n",
    "    else:\n",
    "        # NLP\n",
    "        pro_text= NLP(e.get())\n",
    "        # to find which row has intersection with the words from question you asked\n",
    "        # which means to find the who have common elements between your question and the data\n",
    "        inter_list=[]\n",
    "        for i in cleaned_questions:\n",
    "            if (set(pro_text) & set(i)):\n",
    "                inter_list.append((list(set(pro_text) & set(i)),cleaned_questions.index(i)))         \n",
    "\n",
    "        # remove stop words  \n",
    "        new_inter_list=[]\n",
    "        for i in range(len(inter_list)):\n",
    "            for j in inter_list[i][0]:\n",
    "                if j not in stopwords.words('english'):\n",
    "                    new_inter_list.append(inter_list[i])\n",
    "\n",
    "        # find the max length of common elements\n",
    "        lengths=[len(new_inter_list[i][0]) for i in range(len(new_inter_list)) ]\n",
    "        indexes=[]\n",
    "        if len(lengths)>0:\n",
    "            max_length=max(lengths)\n",
    "            # find all the index whose correspondiong question data have the most common elements\n",
    "            for i in range(len(new_inter_list)):\n",
    "                if len(new_inter_list[i][0])==max_length:\n",
    "                    indexes.append(new_inter_list[i][1])\n",
    "                    # to find ratios that the keys in a sentence\n",
    "            ratios=[]\n",
    "            for i in list(set(indexes)):\n",
    "                ratio=len(pro_text)/len(questions.iloc[i])\n",
    "                ratios.append((ratio,i))\n",
    "            final_indexes=[]\n",
    "            if [ratios[i][0] for i in range(len(ratios))]!=[]:\n",
    "                max_ratios=max([ratios[i][0] for i in range(len(ratios))])\n",
    "\n",
    "                for i in range(len(ratios)):\n",
    "                    if ratios[i][0]==max_ratios:\n",
    "                        final_indexes.append(ratios[i][1])\n",
    "            else:\n",
    "                txt.insert(END,\"\\n\"+\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"+\"\\n\")\n",
    "                e.delete(0,END)\n",
    "                txt.see(END)\n",
    "\n",
    "\n",
    "\n",
    "            if len(final_indexes)>0:\n",
    "                # to randomly find an answer based on the index\n",
    "                answer_index=random.choice(final_indexes)\n",
    "                txt.insert(END,\"\\n\"+ \"Chacha: \"+ answers.iloc[answer_index]+\"\\n\")\n",
    "                e.delete(0,END)\n",
    "                txt.see(END)\n",
    "            else:\n",
    "                txt.insert(END,\"\\n\"+\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"+\"\\n\")\n",
    "                e.delete(0,END)\n",
    "                txt.see(END)        \n",
    "                    \n",
    "        else:\n",
    "            txt.insert(END,\"\\n\"+\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"+\"\\n\")\n",
    "            e.delete(0,END)\n",
    "            txt.see(END)\n",
    "\n",
    " \n",
    "    e.delete(0,END)\n",
    "    \n",
    "    \n",
    "root.bind('<Return>', send)    \n",
    "txt=Text(root)\n",
    "txt.grid(row=0,column=0,columnspan=2)\n",
    "e=Entry(root,width=60)\n",
    "send=Button(root,text=\"Send\",command=send,fg='black', bg='white').grid(row=1,column=1)\n",
    "e.grid(row=1,column=0)\n",
    "root.title(\"Mini Chatbot for Data Science\")\n",
    "# root.configure(bg='white')\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
