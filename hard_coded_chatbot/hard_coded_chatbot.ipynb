{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refereces:\n",
    "- https://www.google.com/search?q=how+to+use+tkinter+to+create+a+chatbot+in+python&oq=how+to+use+tkinter+to+create+a+chatbot&aqs=chrome.1.69i57j33.44216j0j4&sourceid=chrome&ie=UTF-8#kpvalbx=_VJZOX5znAquIytMPkfS2iAE37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages/libraries first \n",
    "# !pip install pyttsx3\n",
    "# !pip install wikipedia\n",
    "# !pip install SpeechRecognition\n",
    "# !pip install pygame\n",
    "# !pip install pyown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyaudio\n",
    "# !pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import *\n",
    "# root=Tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "### import libraries for voice output\n",
    "# source: https://www.codementor.io/@edwardzionsaji/simple-voice-enabled-chat-bot-in-python-kt2qi5oke\n",
    "import datetime\n",
    "import webbrowser\n",
    "import pyttsx3\n",
    "import wikipedia\n",
    "from pygame import mixer\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and calibrate the text to speech engine.\n",
    "# Now we need to set the voice rate, engine, etc.\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[0].id)\n",
    "volume = engine.getProperty('volume')\n",
    "engine.setProperty('volume', 10.0)\n",
    "rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', rate - 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "df=pd.read_csv(\"chatbot_data_Q&A - basic_python_questions.csv\").dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "answer      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()# check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data?</td>\n",
       "      <td>Based on the definition from google : facts an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is data science?</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is big data?</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>definition of big data</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer\n",
       "0                    data?  Based on the definition from google : facts an...\n",
       "1    what is data science?  Data science is an inter-disciplinary field th...\n",
       "2             data science  Data science is an inter-disciplinary field th...\n",
       "3        what is big data?  In the data science domain, big data usually r...\n",
       "4  definition of big data   In the data science domain, big data usually r..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()# take a look at the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some uncompleted questions\n",
    "# if a question or answers end with \":\" \n",
    "questions=[]\n",
    "for i in df.question:\n",
    "    if i[-1]==\":\":\n",
    "        questions.append(None)\n",
    "    else:\n",
    "        questions.append(i)\n",
    "        \n",
    "        \n",
    "# remove some uncompleted answers\n",
    "answers=[]\n",
    "for i in df.answer:\n",
    "    if i[-1]==\":\":\n",
    "        answers.append(None)\n",
    "    else:\n",
    "        answers.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange a little bit\n",
    "df['answer']=answers\n",
    "df['question']=questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data?</td>\n",
       "      <td>Based on the definition from google : facts an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is data science?</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science</td>\n",
       "      <td>Data science is an inter-disciplinary field th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is big data?</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>definition of big data</td>\n",
       "      <td>In the data science domain, big data usually r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer\n",
       "0                    data?  Based on the definition from google : facts an...\n",
       "1    what is data science?  Data science is an inter-disciplinary field th...\n",
       "2             data science  Data science is an inter-disciplinary field th...\n",
       "3        what is big data?  In the data science domain, big data usually r...\n",
       "4  definition of big data   In the data science domain, big data usually r..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(axis=0).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]\\|', \"\", text)\n",
    "    text=text.replace(\"[\\'\",\"\").replace(\"\\n\",\" \").replace(\"']\",\" \").replace('[\"',\"\").replace('\"]',\"\").replace(\"it\\'s\",\"it's \").replace(\"\\', \\'\",\"\")\n",
    "    text=text.replace(\"\\',\",\"\").replace(\"it\\'s\",\"it is \").replace(\"it\\\\\\'s\",\"it is\").replace(\" \\\\\",\" \")\n",
    "    text=text.replace('\",',\" \").replace(\"\\',\",\"\").replace( \":\\',\",\"\").replace(\"here\\'s\",\"\").replace(\":\",\"\").replace(',\"',\"\")\n",
    "    text=text.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'s\",\"'s\")\n",
    "    text=re.sub(r\"let's\", \"let us\", text)\n",
    "    text = text.replace(\"\\'s\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#declare answers and questions\n",
    "questions=df.question\n",
    "answers=df.answer\n",
    "# Cleaning the questions\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "# Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definition of data visualization, what is data visualization?',\n",
       " 'what tools do professional data scientists use? ',\n",
       " 'mian tools in data science',\n",
       " 'how to install git',\n",
       " 'what is conda and anaconda?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_questions[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contains  punctuation\n",
    "#sw_list = stopwords.words('english')\n",
    "sw_list = list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'said',\"'s\", \"also\",'one',\"n't\",'com', '-', '–','--' ,\n",
    "'—', '_']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    return punctuation_removed\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "# import libraries\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    #tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    # stemming\n",
    "    stemming_string=list(map(stemming,processed_string))\n",
    "    # lemmatization\n",
    "    lemma_string=list(map(lemmatization,stemming_string))\n",
    "    \n",
    "    return lemma_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function above to process the data \n",
    "cleaned_questions=data_preprocessing(clean_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['data'], ['what', 'be', 'data', 'scienc'], ['data', 'scienc'], ['what', 'be', 'big', 'data'], ['definit', 'of', 'big', 'data'], ['big', 'data'], ['famou', 'data', 'scientist'], ['what', 'be', 'matplotlib'], ['what', 'be', 'seaborn'], ['what', 'be', 'panda']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_questions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function of NLP for single line\n",
    "def NLP(text):\n",
    "    cleaned_question=clean_text(text)\n",
    "    processed_question=process_data(cleaned_question)\n",
    "    stemming_question=stemming(processed_question)\n",
    "    lemma_question=lemmatization(stemming_question)\n",
    "    return lemma_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Greeting , goodbye and thank you you are welcome\n",
    "greeting=['hey', 'hi', 'hello', 'hey man', 'hi how are you', 'how are you','how is it going', \n",
    "          'nice to meet you', 'how are you doing', 'what is up', 'what is new', \n",
    "          'what is going on', 'how is everything', 'how are things', 'how is life', \n",
    "          'how is your day', 'how is your day going', 'good to see you', 'nice to see you']\n",
    "goodbye=[\"see you\",\"bye\",\"byebye\",\"goodbye\"]\n",
    "\n",
    "thankyou=[\"thanks\",\"thank you\", \"thank you very much\"]\n",
    "yourwelcome=[\"you are welcome ^.^\",\"my pleasure!\",\"I am happy to help you!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease run part 1 and part 2 individually.\\nIf you run part 1 first and run part 2 please \" comment\" part 1 then part 2\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=============================================================================================================#\n",
    "'''\n",
    "Please run part 1 and part 2 individually.\n",
    "If you run part 1 first and run part 2 please \" comment\" part 1 then part 2\n",
    "'''\n",
    "#=============================================================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One - With Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# function - chatbot\n",
    "def chatbot(question):\n",
    "   \n",
    "    if  question.strip().lower() in greeting:\n",
    "        answer=random.choice(greeting).capitalize()\n",
    "        return  answer\n",
    "        \n",
    "        \n",
    "    elif question.strip().lower() in thankyou:\n",
    "        answer= random.choice(yourwelcome).capitalize()\n",
    "        return  answer\n",
    "    else:\n",
    "        # NLP\n",
    "        pro_text= NLP(question)\n",
    "\n",
    "        # to find which row has intersection with the words from question you asked\n",
    "        # which means to find the who have common elements between your question and the data\n",
    "        inter_list=[]\n",
    "        for i in cleaned_questions:\n",
    "            if (set(pro_text) & set(i)):\n",
    "                inter_list.append((list(set(pro_text) & set(i)),cleaned_questions.index(i)))\n",
    "\n",
    "        # remove stop words  \n",
    "        new_inter_list=[]\n",
    "        for i in range(len(inter_list)):\n",
    "            for j in inter_list[i][0]:\n",
    "                if j not in stopwords.words('english'):\n",
    "                    new_inter_list.append(inter_list[i])\n",
    "\n",
    "        # find the max length of common elements\n",
    "        lengths=[len(new_inter_list[i][0]) for i in range(len(new_inter_list)) ]\n",
    "        \n",
    "\n",
    "        indexes=[]\n",
    "        if len(lengths)>0:\n",
    "            max_length=max(lengths)\n",
    "            # find all the index whose correspondiong question data have the most common elements\n",
    "            for i in range(len(new_inter_list)):\n",
    "                if len(new_inter_list[i][0])==max_length:\n",
    "                    indexes.append(new_inter_list[i][1])\n",
    "        else:\n",
    "            return \"Sorry, I don't know. I need to learn more!\"\n",
    "\n",
    "\n",
    "\n",
    "        ratios=[]\n",
    "        for i in list(set(indexes)):\n",
    "            ratio=len(pro_text)/len(questions.iloc[i])\n",
    "            ratios.append((ratio,i))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        final_indexes=[]\n",
    "        if [ratios[i][0] for i in range(len(ratios))]!=0:\n",
    "            max_ratios=max([ratios[i][0] for i in range(len(ratios))])\n",
    "            for i in range(len(ratios)):\n",
    "                if ratios[i][0]==max_ratios:\n",
    "                    final_indexes.append(ratios[i][1])\n",
    "\n",
    "\n",
    "        if len(final_indexes)>0:\n",
    "            # to randomly find an answer based on the index\n",
    "            answer_index=random.choice(final_indexes)\n",
    "            answer=answers.iloc[answer_index]    \n",
    "            return answer\n",
    "\n",
    "        else:\n",
    "            return \"Sorry, I don't know. I need to learn more!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============# Hi There my name is Chacha. Let's Talk #=============\n",
      "You: hi\n",
      "Chacha: What is up\n",
      "\n",
      "You: what is data science?\n",
      "Chacha: Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms ... in managing a digital data collection. There is still no consensus on the definition of data science and it is considered by some to be a buzzword.\n",
      "\n",
      "You: how to select a column from a dataframe?\n",
      "Chacha: # if you want to select a specific column you can use this code:\n",
      "df[\"column_name\"] \n",
      "\n",
      "You: how to install git\n",
      "Chacha: Git insallation on Windows: https://www.youtube.com/watch?v=2j7fD92g-gE\n",
      "Git insallation on Mac: https://www.youtube.com/watch?v=sJ4zr0a4GAs\n",
      "\n",
      "You: what is normal distribution?\n",
      "Chacha: The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.\n",
      "\n",
      "You: thanks\n",
      "Chacha: I am happy to help you!\n",
      "\n",
      "You: byebye\n",
      "Chacha: bye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=============# Hi There my name is Chacha. Let's Talk #=============\")\n",
    "\n",
    "while(True): \n",
    "    question = input(\"You: \")\n",
    "    if question.strip().lower() in goodbye:\n",
    "        answer=random.choice(goodbye)\n",
    "        print(\"Chacha: \"+answer+\"\\n\") \n",
    "        engine.say(answer)\n",
    "        engine.runAndWait()\n",
    "        break\n",
    "    else:\n",
    "        pro_text= NLP(question)\n",
    "        answer=chatbot(question)\n",
    "        print(\"Chacha: \"+answer+\"\\n\")\n",
    "        engine.say(answer)\n",
    "        engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two - chatbot with interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def func(event):\n",
    "#      print(\"You hit return.\")\n",
    "# def send(event=None):\n",
    "#     send=\"You: \"+e.get()\n",
    "#     txt.insert(END,\"\\n\"+send)\n",
    "#     txt.see(END+\"\\n\")\n",
    "    \n",
    "#     if e.get().strip().lower() in goodbye:\n",
    "#         byebye=\"Byebye! see you next time! \"\n",
    "#         txt.insert(END,\"\\n\"+\"Chacha: \"+byebye+\"\\n\")  \n",
    "#         e.delete(0,END)\n",
    "#         txt.see(END)\n",
    "\n",
    "#     elif  e.get().strip().lower() in greeting:\n",
    "#         answer=random.choice(greeting).capitalize()\n",
    "#         txt.insert(END,\"\\n\"+\"Chacha: \"+ answer+\"\\n\")\n",
    "#         e.delete(0,END)\n",
    "#         txt.see(END)\n",
    "\n",
    "\n",
    "#     elif e.get().strip().lower() in thankyou:\n",
    "#         txt.insert(END,\"\\n\"+\"Chacha: \"+ random.choice(yourwelcome).capitalize()+\"\\n\")\n",
    "#         e.delete(0,END)\n",
    "#         txt.see(END)\n",
    "#     else:\n",
    "#         # NLP\n",
    "#         pro_text= NLP(e.get())\n",
    "#         # to find which row has intersection with the words from question you asked\n",
    "#         # which means to find the who have common elements between your question and the data\n",
    "#         inter_list=[]\n",
    "#         for i in cleaned_questions:\n",
    "#             if (set(pro_text) & set(i)):\n",
    "#                 inter_list.append((list(set(pro_text) & set(i)),cleaned_questions.index(i)))         \n",
    "\n",
    "#         # remove stop words  \n",
    "#         new_inter_list=[]\n",
    "#         for i in range(len(inter_list)):\n",
    "#             for j in inter_list[i][0]:\n",
    "#                 if j not in stopwords.words('english'):\n",
    "#                     new_inter_list.append(inter_list[i])\n",
    "\n",
    "#         # find the max length of common elements\n",
    "#         lengths=[len(new_inter_list[i][0]) for i in range(len(new_inter_list)) ]\n",
    "#         indexes=[]\n",
    "#         if len(lengths)>0:\n",
    "#             max_length=max(lengths)\n",
    "#             # find all the index whose correspondiong question data have the most common elements\n",
    "#             for i in range(len(new_inter_list)):\n",
    "#                 if len(new_inter_list[i][0])==max_length:\n",
    "#                     indexes.append(new_inter_list[i][1])\n",
    "#                     # to find ratios that the keys in a sentence\n",
    "#             ratios=[]\n",
    "#             for i in list(set(indexes)):\n",
    "#                 ratio=len(pro_text)/len(questions.iloc[i])\n",
    "#                 ratios.append((ratio,i))\n",
    "#             final_indexes=[]\n",
    "#             if [ratios[i][0] for i in range(len(ratios))]!=[]:\n",
    "#                 max_ratios=max([ratios[i][0] for i in range(len(ratios))])\n",
    "\n",
    "#                 for i in range(len(ratios)):\n",
    "#                     if ratios[i][0]==max_ratios:\n",
    "#                         final_indexes.append(ratios[i][1])\n",
    "#             else:\n",
    "#                 txt.insert(END,\"\\n\"+\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"+\"\\n\")\n",
    "#                 e.delete(0,END)\n",
    "#                 txt.see(END)\n",
    "\n",
    "\n",
    "\n",
    "#             if len(final_indexes)>0:\n",
    "#                 # to randomly find an answer based on the index\n",
    "#                 answer_index=random.choice(final_indexes)\n",
    "#                 txt.insert(END,\"\\n\"+ \"Chacha: \"+ answers.iloc[answer_index]+\"\\n\")\n",
    "#                 e.delete(0,END)\n",
    "#                 txt.see(END)\n",
    "#             else:\n",
    "#                 txt.insert(END,\"\\n\"+\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"+\"\\n\")\n",
    "#                 e.delete(0,END)\n",
    "#                 txt.see(END)        \n",
    "                    \n",
    "#         else:\n",
    "#             txt.insert(END,\"\\n\"+\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"+\"\\n\")\n",
    "#             e.delete(0,END)\n",
    "#             txt.see(END)\n",
    "\n",
    " \n",
    "#     e.delete(0,END)\n",
    "    \n",
    "    \n",
    "# root.bind('<Return>', send)    \n",
    "# txt=Text(root)\n",
    "# txt.grid(row=0,column=0,columnspan=2)\n",
    "# e=Entry(root,width=60)\n",
    "# send=Button(root,text=\"Send\",command=send,fg='black', bg='white').grid(row=1,column=1)\n",
    "# e.grid(row=1,column=0)\n",
    "# root.title(\"Mini Chatbot for Data Science\")\n",
    "# # root.configure(bg='white')\n",
    "# root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
